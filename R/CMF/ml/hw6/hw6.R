#–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –≤ R
# —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ä–∞–∑–º–µ—Ä —Å–ª–æ—ë–≤)
source("C:/Users/user/Desktop/proga/R/cmf/learn/hw6/NN_functions.R")
model <- list(layer = c(2, 30, 30, 3), activation = c("sigmoid",
                                                      "sigmoid", "sigmoid")); L <- unlist(lapply(model, length))
# –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Œò
#ùëô–≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º
# –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—è–≤–ª–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
Theta <- InitPar(model, eps = 1)
# —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏




DATX = read.csv("C:/Users/user/Desktop/proga/R/cmf/learn/hw6/train.csv",header=TRUE,stringsAsFactors = FALSE,sep=",")
X=cbind(DATX[,1],DATX[,2])

z=max(y)
y=NULL

for(i in 1:m){
  if(DATX[i,3]==1){
    y=rbind(y,c(1,0,0))
  }
  else if (DATX[i,3]==2) {
    y=rbind(y,c(0,1,0))
  }
  else{
    y=rbind(y,c(0,0,1))
  }
}

m=5000
m.train <- trunc(0.8*m); m.test <- m - m.train
X.train <- X[1:m.train,]; X.test <- X[(m.train+1):m,]
y.train <- y[1:m.train,]; y.test <- y[(m.train+1):m,]

#–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
#–ü—É—Å—Ç—å —Å—Ç–æ–∏—Ç –∑–∞–¥–∞—á–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –Ω–∞ —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞, –¥–ª—è
#—á–µ–≥–æ –∏–º–µ—é—Ç—Å—è –¥–≤–µ –æ–±—ä—è—Å–Ω—è—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —Ç–æ–≥–¥–∞ –º–∞—Ç—Ä–∏—Ü—ã
#–∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –≤–∏–¥
head(X)
head(y)
#–ï–¥–∏–Ω–∏—Ü—ã –≤ —Å—Ç—Ä–æ–∫–∞—Ö –º–∞—Ç—Ä–∏—Ü—ã y –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å
#–Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∫ –æ–¥–Ω–æ–º—É –∏–∑ —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–∞–∂–¥–æ–º—É –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö
#—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã

#Forward propagation
predictNN <- function(X, Theta, model, min.val = 10^-7) {
  
  L <- unlist(lapply(model, length))
  if (length(L) != 2) stop("In predictNN: invalid model")
  if (L[1] != L[2] + 1) stop("In predictNN: number of layers and number
                             of activations don't match")
  if (L[2] < 2) stop("network must have at least 2 layers")
  if (!all(model$activation %in% c("sigmoid", "relu", "softmax")))
    stop("In predictNN: activation must be in ('sigmoid', 'relu',
         'softmax')")
  act <- vector("list", L[2])
  for (i in 1:L[2]) {
    if (model$activation[i] == "relu") {
      act[[i]] <- relu
    } else if (model$activation[i] == "sigmoid") {
      act[[i]] <- sigmoid
    } else if (model$activation[i] == "softmax") {
      act[[i]] <- softmax
    }
  }
  
  # your code here
  list(a = a, z = z)
}


#–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
J <- function(X, y, theta, model, lambda = 0, min.val = 10^-7) {
  L <- unlist(lapply(model, length))
  if (length(L) != 2) stop("In J: invalid model")
  if (L[1] != L[2] + 1) stop("In J: number of layers and number of
                             activations don't match")
  if (L[2] < 2) stop("network must have at least 2 layers")
  m <- nrow(X)
  Theta <- unvec(theta, model)
  aL <- predictNN(X, Theta, model)$a[[L[1]]]
  aL[aL < min.val] <- min.val
  aL[aL > 1 - min.val] <- 1 - min.val
  err <- -sum(apply(y * log(aL), 1, sum) + apply((1 - y) * log(1 -
                                                                 aL), 1, sum)) / m
  reg <- lambda / (2*m) * sum(theta^2)
  err + reg
}

#Back propagation (–≥—Ä–∞–¥–∏–µ–Ω—Ç)
gradJ <- function(X, y, theta, model, lambda = 0) {
  m <- nrow(X); L <- unlist(lapply(model, length))
  # you may check if model is valid
  if (!all(model$activation %in% c("sigmoid", "relu", "softmax")))
    stop("In gradJ: activation must be in ('sigmoid', 'relu', 'softmax')")
  act.grad <- vector("list", L[2])
  for (i in 1:L[2]) {
    if (model$activation[i] == "relu") {
      act.grad[[i]] <- relu.grad
    } else if (model$activation[i] == "sigmoid") {
      act.grad[[i]] <- sigmoid.grad
    } else if (model$activation[i] == "softmax") {
      act.grad[[i]] <- softmax.grad
    }
  }
  Theta <- unvec(theta, model)
  pred <- predictNN(X, Theta, model)
  delta <- vector("list", L[1])
  Delta <- reg <- vector("list", L[2])
  # your code here
  
  vec(Delta)
  
}
#–ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
lambda <- 0.1 # –Ω–∞–ø—Ä–∏–º–µ—Ä
# —Ä–∞–∑–≤—ë—Ä—Ç–∫–∞ –º–∞—Ç—Ä–∏—Ü Œò
# –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç
theta <- vec(Theta, model)
# —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, –∑–∞–≤–∏—Å—è—â–∏–µ —Ç–æ–ª—å–∫–æ –æ—Ç theta
err <- function(x) J(X.train, y.train, x, model, lambda)
gr <- function(x) gradJ(X.train, y.train, x, model, lambda)
# –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –æ—à–∏–±–∫–∏ (–Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ)
opt <- optim(fn=errFunc, gr=gr, par=theta, method="BFGS")
# —Å–≤—ë—Ä—Ç–∫–∞ –º–∞—Ç—Ä–∏—Ü Œò
Theta <- unvec(opt$par, model)
#–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞
# –∫–ª –∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–ø—Ä–æ–≥–Ω–æ–∑) –Ω–∞ —ç–∫–∑–∞–º–µ–Ω—É—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
y.pred <- predictNN(X.test, Theta, model)$a[[L[1]]]
#–ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç —Å–ª–µ–¥—É—é—â–∏–π –≤–∏–¥:
head(y.pred)
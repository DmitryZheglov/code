{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_sum(l, r, p=1.0):\n",
    "    return np.sum(np.arrange(l, r + 1, 1)**p)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def solve_equation(a, b, c):\n",
    "    EPS = 10**(-10)\n",
    "    D = b**2 - 4*a*c\n",
    "    print(D)\n",
    "    if(D > EPS):\n",
    "        x1 = (-b - np.sqrt(D)) / (2*a)\n",
    "        x2 = (-b + np.sqrt(D)) / (2*a)\n",
    "        return [x1, x2]\n",
    "    elif(abs(D) < EPS):\n",
    "        return (-b) / (2*a)\n",
    "    else:\n",
    "        return np.inf\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_outliers(x, std_mul=3.0):\n",
    "    m = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    print(m,s*3)\n",
    "    return np.where(abs(x - m) > std_mul * s, m, x)\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eigenvector(A, alpha):\n",
    "    EPS = 10**(-6)\n",
    "    s = np.linalg.eig(A)\n",
    "    for i in range(len(A)):\n",
    "        if abs(s[0][i] - alpha) < EPS:\n",
    "            return s[1][i]\n",
    "        \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_sampler(p):\n",
    "    return np.random.choice(np.arange(len(p)),1,p = p)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(x, mu=0.0, sigma=1.0):\n",
    "    return np.sum(-np.log(sigma*np.sqrt(2*np.pi))-((x-mu)**2)/(2*(sigma**2)))\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_approx(f, x0, eps=1e-8):\n",
    "    return (f(x0 + eps) - f(x0 - eps))/(2*eps)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_method(f, x0, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "    x_k = x0\n",
    "    for i in range(n_steps):\n",
    "        x_k = x_k - gradient_approx(f, x_k, eps)*learning_rate\n",
    "    return (f(x_k), x_k)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_predict(w, b, X):\n",
    "    return np.dot(X,w)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_mse_gradient(w, b, X, y_true):\n",
    "    return 2*(y_true-np.dot(X,w))\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegressor:\n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        self.b = 1\n",
    "        for i in range(n_steps):\n",
    "            self.w = self.w - learning_rate*linear_regression_mse_gradient(self.w, self.b, X_train, y_train)\n",
    "\n",
    "        raise NotImplementedError\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return linear_regression_predict(self.w, self.b, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    return np.exp(-x) / ((1.0 + np.exp(-x))**2)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_der(x):\n",
    "    EPS = 10**(-6)\n",
    "    alpha = 0\n",
    "    if x > EPS:\n",
    "        return 1\n",
    "    elif x < EPS:\n",
    "        return 0\n",
    "    else:\n",
    "        return aplha #alpha in [0,1]\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7760129 ,  1.        ,  1.09039374])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform([-1, 1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPRegressor:\n",
    "\t\"\"\"\n",
    "\t\tsimple dense neural network class for regression with mse loss. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_units=[32, 32], nonlinearity=relu):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: n_units - number of neurons for each hidden layer in neural network,\n",
    "\t\t\t       nonlinearity - activation function applied between hidden layers.\n",
    "\t\t\"\"\"\n",
    "        \n",
    "        self.n_units = n_units\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        b = 1\n",
    "        self.n_neur = [X_train.shape[1]]\n",
    "        self.n_neur.extend(self.n_units)\n",
    "        self.w = []\n",
    "        for k in range(len(self.n_units)):\n",
    "            for l in range(self.n_units[k]):\n",
    "                self.w.append(np.random.uniform([-1/self.n_units[k],1/self.n_units[k],self.n_neur[k]]))\n",
    "        l = len(X_train)\n",
    "        M = X_train.shape[1]\n",
    "        for step in range(n_steps):\n",
    "            i = np.random.randint(l, size=1)\n",
    "            x_i = X_train[i][:]\n",
    "            for k in range(len(self.n_units)):\n",
    "                u_i = np.zeros(self.n_units[k])\n",
    "                a_i = np.zeros(M)\n",
    "                e_i = np.zeros(M)\n",
    "                e_i_h = np.zeros(self.n_units[k])\n",
    "                for h in range(self.n_units[k]):\n",
    "                    u_i[h] = relu(np.dot(x_i,self.w[k+h]))\n",
    "                for m in range(M):\n",
    "                    a_i[m] = relu(np.dot(u_i,self.w[h+m]))\n",
    "                    e_i[m] = a_i[m] - y_i[m]\n",
    "                Q_i = np.sum(e_i**2)\n",
    "                for h in range(self.n_units[k]):\n",
    "                    e_i_h[h] = np.sum(e_i*relu_der(self.w[k+h]))\n",
    "                for h in range(self.n_units[k]):\n",
    "                    for m in range(M):\n",
    "                        self.w[h+m] = self.w[h+m] - learning_rate * e_i[m]*relu_der(u_h)\n",
    "                for j in range(l):\n",
    "                    for h in range(self.n_units[k]):\n",
    "                        self.w[h+j] = self.w[h+j] - learning_rate * e_i[h]*relu_der(X_train[j][:])\n",
    "            Q = Q*(l-1)/l+Q_i/l\n",
    "                    \n",
    "        \n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix and targets.\n",
    "\t\t\toptimises mse w.r.t model parameters\n",
    "\t\t\t(you may use approximate gradient estimation)\n",
    "\t\t\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\tdef predict(self, X):\n",
    "        return np.dot(X,self.w[-1])\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix\n",
    "\t\t\treturns MLP predictions in X\n",
    "\t\t\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPRegressor:\n",
    "\t\"\"\"\n",
    "\t\tsimple dense neural network class for regression with mse loss. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_units=[32, 32], nonlinearity=relu):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: n_units - number of neurons for each hidden layer in neural network,\n",
    "\t\t\t       nonlinearity - activation function applied between hidden layers.\n",
    "\t\t\"\"\"\n",
    "        \n",
    "        self.n_units = n_units\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        b = 1\n",
    "        self.n_neur = [X_train.shape[1]]\n",
    "        self.n_neur.extend(self.n_units)\n",
    "        self.w = []\n",
    "        for k in range(len(self.n_units)):\n",
    "            for l in range(self.n_units[k]):\n",
    "                self.w.append(np.random(self.n_neur[k]) / 1000)\n",
    "        X = X_train\n",
    "        for i in range(n_steps):\n",
    "            for k in range(len(self.n_units)):\n",
    "                for l in range(self.n_units[k]):\n",
    "                    X[:][l] = relu(np.dot(X,self.w[k+l]))\n",
    "                    \n",
    "        for \n",
    "                    self.w = self.w - learning_rate * gradient_approx(np.max((y-np.dot(X_train,self.w))**2,0), self.w, eps=1e-8)\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix and targets.\n",
    "\t\t\toptimises mse w.r.t model parameters\n",
    "\t\t\t(you may use approximate gradient estimation)\n",
    "\t\t\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\tdef predict(self, X):\n",
    "        return np.dot(X,self.w)\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix\n",
    "\t\t\treturns MLP predictions in X\n",
    "\t\t\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf8 -*-\n",
    "\n",
    "\n",
    "####################################################\n",
    "### You are not allowed to import anything else. ###\n",
    "####################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def power_sum(l, r, p=1.0):\n",
    "\t\"\"\"\n",
    "\t\tinput: l, r - integers, p - float\n",
    "\t\treturns sum of p powers of integers from [l, r]\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def solve_equation(a, b, c):\n",
    "\t\"\"\"\n",
    "\t\tinput: a, b, c - integers\n",
    "\t\treturns float solutions x of the following equation: a x ** 2 + b x + c == 0\n",
    "\t\t\tIn case of two diffrent solution returns tuple / list (x1, x2)\n",
    "\t\t\tIn case of one solution returns one float\n",
    "\t\t\tIn case of no float solutions return None \n",
    "\t\t\tIn case of infinity number of solutions returns 'inf'\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def replace_outliers(x, std_mul=3.0):\n",
    "\t\"\"\"\n",
    "\t\tinput: x - numpy vector, std_mul - positive float\n",
    "\t\treturns copy of x with all outliers (elements, which are beyond std_mul * std from mean) replaced with mean  \n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def get_eigenvector(A, alpha):\n",
    "\t\"\"\"\n",
    "\t\tinput: A - square numpy matrix, alpha - float\n",
    "\t\treturns numpy vector - eigenvector of A corresponding to eigenvalue alpha.\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def discrete_sampler(p):\n",
    "\t\"\"\"\n",
    "\t\tinput: p - numpy vector of probability (non-negative, sums to 1)\n",
    "\t\treturns integer from 0 to len(p) - 1, each integer i is returned with probability p[i] \n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def gaussian_log_likelihood(x, mu=0.0, sigma=1.0):\n",
    "\t\"\"\"\n",
    "\t\tinput: x - numpy vector, mu - float, sigma - positive float\n",
    "\t\treturns log p(x| mu, sigma) - log-likelihood of x dataset \n",
    "\t\tin univariate gaussian model with mean mu and standart deviation sigma\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def gradient_approx(f, x0, eps=1e-8):\n",
    "\t\"\"\"\n",
    "\t\tinput: f - callable, function of vector x. x0 - numpy vector, eps - float, represents step for x_i\n",
    "\t\treturns numpy vector - gradient of f in x0 calculated with finite difference method \n",
    "\t\t(for reference use https://en.wikipedia.org/wiki/Numerical_differentiation, search for \"first-order divided difference\")\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def gradient_method(f, x0, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "\t\"\"\"\n",
    "\t\tinput: f - function of x. x0 - numpy vector, n_steps - integer, learning rate, eps - float.\n",
    "\t\treturns tuple (f^*, x^*), where x^* is local minimum point, found after n_steps of gradient descent, \n",
    "\t\t                                f^* - resulting function value.\n",
    "\t\tImpletent gradient descent method, given in the lecture. \n",
    "\t\tFor gradient use finite difference approximation with eps step.\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def linear_regression_predict(w, b, X):\n",
    "\t\"\"\"\n",
    "\t\tinput: w - numpy vector of M weights, b - bias, X - numpy matrix N x M (object-feature matrix), \n",
    "\t\tN - number of objects, M - number of features.\n",
    "\t\treturns numpy vector of predictions of linear regression model for X\n",
    "\t\thttps://xkcd.com/1725/\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\t\tinput: two numpy vectors of object targets and model predictions.\n",
    "\t\treturn mse\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def linear_regression_mse_gradient(w, b, X, y_true):\n",
    "\t\"\"\"\n",
    "\t\tinput: w, b - weights and bias of a linear regression model,\n",
    "\t\t        X - object-feature matrix, y_true - targets.\n",
    "\t\treturns gradient of linear regression model mean squared error w.r.t x and b\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "\tdef fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix and targets.\n",
    "\t\t\toptimises mse w.r.t model parameters \n",
    "\t\t\"\"\"\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\treturn linear_regression_predict(self.w, self.b, X)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_der(x):\n",
    "\t\"\"\"\n",
    "\t\treturns sigmoid derivative w.r.t. x\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "\treturn np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def relu_der(x):\n",
    "\t\"\"\"\n",
    "\t\treturn relu (sub-)derivative w.r.t x\n",
    "\t\"\"\"\n",
    "\traise NotImplementedError\n",
    "\n",
    "\n",
    "class MLPRegressor:\n",
    "\t\"\"\"\n",
    "\t\tsimple dense neural network class for regression with mse loss. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_units=[32, 32], nonlinearity=relu):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: n_units - number of neurons for each hidden layer in neural network,\n",
    "\t\t\t       nonlinearity - activation function applied between hidden layers.\n",
    "\t\t\"\"\"\n",
    "\t\tself.n_units = n_units\n",
    "\t\tself.nonlinearity = nonlinearity\n",
    "\n",
    "\n",
    "\tdef fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix and targets.\n",
    "\t\t\toptimises mse w.r.t model parameters\n",
    "\t\t\t(you may use approximate gradient estimation)\n",
    "\t\t\"\"\"\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput: object-feature matrix\n",
    "\t\t\treturns MLP predictions in X\n",
    "\t\t\"\"\"\n",
    "\t\traise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

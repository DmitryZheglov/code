{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Functions \n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>\n",
    "<img src=\"img/bp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"./img/loss.png\" width=\"300\">\n",
    "<img src=\"./img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    #reg = 1\n",
    "    \n",
    "    \n",
    "    f_i = x[np.arange(len(x)), y]\n",
    "    f_sum = np.sum(np.exp(x), axis = 1)\n",
    "    loss = np.sum(-f_i + np.log(f_sum)) / len(x)\n",
    "    \n",
    "    f_sum = np.sum(np.exp(x), axis = 1, keepdims = True)\n",
    "    mask = np.zeros_like(x)\n",
    "    mask[np.arange(len(x)), y] = 1\n",
    "    dx = -(mask - np.exp(x) / f_sum) / len(x)\n",
    "\n",
    "   \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      " 1.0402848592931668\n"
     ]
    }
   ],
   "source": [
    "print ('loss is a scalar\\n', loss(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      " [ 0.02724988 -0.06131146  0.03406158  0.02053912 -0.06239301  0.04185389\n",
      "  0.03304523  0.0344869  -0.06753213  0.04237972  0.02650211 -0.06888183\n",
      "  0.0209521   0.04701141 -0.06796351  0.03326831 -0.07526779  0.04199948\n",
      "  0.04117061 -0.08098566  0.03981505  0.02464471  0.03233042 -0.05697514\n",
      " -0.05751179  0.02989984  0.02761195  0.04385635  0.02405185 -0.0679082 ]\n"
     ]
    }
   ],
   "source": [
    "print( 'gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 8.390872708078433e-08\n"
     ]
    }
   ],
   "source": [
    "print('difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(10, 6).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    N = len(x)\n",
    "    D = np.product(x.shape[1:])\n",
    "    out = (np.dot(x.reshape(N, D), w) + b)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    \n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769847728806635e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    D = np.product(x.shape[1:])\n",
    "    N = len(x)\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    dw = np.dot(x.reshape(N, D).T, dout)\n",
    "    db = np.sum(dout, axis = 0)\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.0435867027554953e-08\n",
      "dw error:  1.05022570882252e-10\n",
      "db error:  1.8439327395560747e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print( 'dx error: ', rel_error(dx_num, dx))\n",
    "print( 'dw error: ', rel_error(dw_num, dw))\n",
    "print( 'db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = np.maximum(0, x)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    mask = x > 0\n",
    "    dx = dout * mask\n",
    "    #dx[~mask] = 0 \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756228186800854e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzheglov/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/dzheglov/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzheglov/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9593d3dcf8>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f959a1d7c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "\t tr_loss 2.30\n",
      "\t te_loss 2.31\n",
      "\t te_acc 0.0962962962962963\n",
      "epoch 1000:\n",
      "\t tr_loss 2.29\n",
      "\t te_loss 2.30\n",
      "\t te_acc 0.07962962962962963\n",
      "epoch 2000:\n",
      "\t tr_loss 2.27\n",
      "\t te_loss 2.29\n",
      "\t te_acc 0.09259259259259259\n",
      "epoch 3000:\n",
      "\t tr_loss 2.27\n",
      "\t te_loss 2.28\n",
      "\t te_acc 0.2074074074074074\n",
      "epoch 4000:\n",
      "\t tr_loss 2.23\n",
      "\t te_loss 2.24\n",
      "\t te_acc 0.3851851851851852\n",
      "epoch 5000:\n",
      "\t tr_loss 2.16\n",
      "\t te_loss 2.18\n",
      "\t te_acc 0.4\n",
      "epoch 6000:\n",
      "\t tr_loss 2.09\n",
      "\t te_loss 2.08\n",
      "\t te_acc 0.5259259259259259\n",
      "epoch 7000:\n",
      "\t tr_loss 1.91\n",
      "\t te_loss 1.92\n",
      "\t te_acc 0.6796296296296296\n",
      "epoch 8000:\n",
      "\t tr_loss 1.71\n",
      "\t te_loss 1.71\n",
      "\t te_acc 0.7351851851851852\n",
      "epoch 9000:\n",
      "\t tr_loss 1.48\n",
      "\t te_loss 1.49\n",
      "\t te_acc 0.7462962962962963\n",
      "epoch 10000:\n",
      "\t tr_loss 1.33\n",
      "\t te_loss 1.28\n",
      "\t te_acc 0.7870370370370371\n",
      "epoch 11000:\n",
      "\t tr_loss 1.21\n",
      "\t te_loss 1.11\n",
      "\t te_acc 0.812962962962963\n",
      "epoch 12000:\n",
      "\t tr_loss 0.89\n",
      "\t te_loss 0.96\n",
      "\t te_acc 0.8314814814814815\n",
      "epoch 13000:\n",
      "\t tr_loss 0.79\n",
      "\t te_loss 0.84\n",
      "\t te_acc 0.8388888888888889\n",
      "epoch 14000:\n",
      "\t tr_loss 0.73\n",
      "\t te_loss 0.75\n",
      "\t te_acc 0.8462962962962963\n",
      "epoch 15000:\n",
      "\t tr_loss 0.60\n",
      "\t te_loss 0.68\n",
      "\t te_acc 0.8685185185185185\n",
      "epoch 16000:\n",
      "\t tr_loss 0.55\n",
      "\t te_loss 0.62\n",
      "\t te_acc 0.8703703703703703\n",
      "epoch 17000:\n",
      "\t tr_loss 0.62\n",
      "\t te_loss 0.57\n",
      "\t te_acc 0.8777777777777778\n",
      "epoch 18000:\n",
      "\t tr_loss 0.45\n",
      "\t te_loss 0.52\n",
      "\t te_acc 0.8888888888888888\n",
      "epoch 19000:\n",
      "\t tr_loss 0.46\n",
      "\t te_loss 0.49\n",
      "\t te_acc 0.8962962962962963\n",
      "epoch 20000:\n",
      "\t tr_loss 0.46\n",
      "\t te_loss 0.46\n",
      "\t te_acc 0.8981481481481481\n",
      "epoch 21000:\n",
      "\t tr_loss 0.38\n",
      "\t te_loss 0.43\n",
      "\t te_acc 0.9018518518518519\n",
      "epoch 22000:\n",
      "\t tr_loss 0.30\n",
      "\t te_loss 0.41\n",
      "\t te_acc 0.9074074074074074\n",
      "epoch 23000:\n",
      "\t tr_loss 0.38\n",
      "\t te_loss 0.39\n",
      "\t te_acc 0.9074074074074074\n",
      "epoch 24000:\n",
      "\t tr_loss 0.35\n",
      "\t te_loss 0.37\n",
      "\t te_acc 0.9111111111111111\n",
      "epoch 25000:\n",
      "\t tr_loss 0.37\n",
      "\t te_loss 0.35\n",
      "\t te_acc 0.9148148148148149\n",
      "epoch 26000:\n",
      "\t tr_loss 0.32\n",
      "\t te_loss 0.34\n",
      "\t te_acc 0.9166666666666666\n",
      "epoch 27000:\n",
      "\t tr_loss 0.36\n",
      "\t te_loss 0.32\n",
      "\t te_acc 0.9222222222222223\n",
      "epoch 28000:\n",
      "\t tr_loss 0.30\n",
      "\t te_loss 0.31\n",
      "\t te_acc 0.9222222222222223\n",
      "epoch 29000:\n",
      "\t tr_loss 0.24\n",
      "\t te_loss 0.30\n",
      "\t te_acc 0.9277777777777778\n",
      "epoch 30000:\n",
      "\t tr_loss 0.25\n",
      "\t te_loss 0.29\n",
      "\t te_acc 0.9277777777777778\n",
      "epoch 31000:\n",
      "\t tr_loss 0.26\n",
      "\t te_loss 0.28\n",
      "\t te_acc 0.924074074074074\n",
      "epoch 32000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.27\n",
      "\t te_acc 0.9277777777777778\n",
      "epoch 33000:\n",
      "\t tr_loss 0.19\n",
      "\t te_loss 0.26\n",
      "\t te_acc 0.9314814814814815\n",
      "epoch 34000:\n",
      "\t tr_loss 0.29\n",
      "\t te_loss 0.26\n",
      "\t te_acc 0.9351851851851852\n",
      "epoch 35000:\n",
      "\t tr_loss 0.28\n",
      "\t te_loss 0.25\n",
      "\t te_acc 0.9407407407407408\n",
      "epoch 36000:\n",
      "\t tr_loss 0.20\n",
      "\t te_loss 0.24\n",
      "\t te_acc 0.9444444444444444\n",
      "epoch 37000:\n",
      "\t tr_loss 0.23\n",
      "\t te_loss 0.24\n",
      "\t te_acc 0.9444444444444444\n",
      "epoch 38000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.23\n",
      "\t te_acc 0.9481481481481482\n",
      "epoch 39000:\n",
      "\t tr_loss 0.19\n",
      "\t te_loss 0.23\n",
      "\t te_acc 0.9518518518518518\n",
      "epoch 40000:\n",
      "\t tr_loss 0.15\n",
      "\t te_loss 0.22\n",
      "\t te_acc 0.9555555555555556\n",
      "epoch 41000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.22\n",
      "\t te_acc 0.9555555555555556\n",
      "epoch 42000:\n",
      "\t tr_loss 0.16\n",
      "\t te_loss 0.21\n",
      "\t te_acc 0.9555555555555556\n",
      "epoch 43000:\n",
      "\t tr_loss 0.22\n",
      "\t te_loss 0.21\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 44000:\n",
      "\t tr_loss 0.13\n",
      "\t te_loss 0.20\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 45000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.20\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 46000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.20\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 47000:\n",
      "\t tr_loss 0.26\n",
      "\t te_loss 0.19\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 48000:\n",
      "\t tr_loss 0.11\n",
      "\t te_loss 0.19\n",
      "\t te_acc 0.9574074074074074\n",
      "epoch 49000:\n",
      "\t tr_loss 0.10\n",
      "\t te_loss 0.19\n",
      "\t te_acc 0.9574074074074074\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.random((64, 100))/100, np.random.random(100)/100\n",
    "W2, b2 = np.random.random((100, 10))/100, np.random.random(10)/100\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2)# Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dx2, dw2, db2 = affine_backward(dx , cache3)\n",
    "    dxr = relu_backward(dx2, cache2)\n",
    "    dx1, dw1, db1 = affine_backward(dxr , cache1)\n",
    "    # Updates\n",
    "    W1 = W1 - lr * dw1 \n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dw2\n",
    "    b2 = b2 - lr * db2\n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    #te_loss = 0\n",
    "    # Predict\n",
    "    #y_pred = np.exp(out3) / np.sum(np.exp(out3))#...\n",
    "    #\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        # Predict\n",
    "        out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "        out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "        out3, cache3 = affine_forward(out2, W2, b2)\n",
    "        te_loss, dx = softmax_loss(out3, y_test)\n",
    "        y_pred = np.argmax(np.exp(out3) / np.sum(np.exp(out3)), axis = -1)#...\n",
    "        print('epoch %s:' % i)\n",
    "        print('\\t tr_loss %.2f' % tr_loss)\n",
    "        print('\\t te_loss %.2f' % te_loss)\n",
    "        print('\\t te_acc %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
